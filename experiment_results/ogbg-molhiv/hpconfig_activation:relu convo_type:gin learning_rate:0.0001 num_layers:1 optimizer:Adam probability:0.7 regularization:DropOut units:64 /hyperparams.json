{"activation": "relu", "convo_type": "gin", "learning_rate": 0.0001, "num_layers": 1, "optimizer": "Adam", "probability": 0.7, "regularization": "DropOut", "units": 64}