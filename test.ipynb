{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 11:27:26.322108: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-12 11:27:26.952284: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-12 11:27:26.952313: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-12 11:27:28.911511: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-12 11:27:28.911743: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-12 11:27:28.911757: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/olga/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gridSearch as g\n",
    "import math\n",
    "import batching as b\n",
    "import modularized as m\n",
    "import convL\n",
    "import convo as c\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import gridS\n",
    "import funcy as fy\n",
    "import gin\n",
    "import poolingL\n",
    "from ogb.nodeproppred import NodePropPredDataset\n",
    "from ogb.graphproppred import GraphPropPredDataset\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import experiments as e\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = GraphPropPredDataset(name = 'ogbg-molhiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arxiv = NodePropPredDataset(name='ogbn-arxiv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'activation': 'relu',\n",
       "  'convo_type': 'gin',\n",
       "  'learning_rate': 0.0001,\n",
       "  'num_layers': 1,\n",
       "  'optimizer': 'Adam',\n",
       "  'probability': 0.3,\n",
       "  'regularization': 'DropOut',\n",
       "  'units': 32}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'num_layers': [1, 2, 3, 4, 5, 6],\n",
    "              'learning_rate': [0.0001, 0.001, 0.01],\n",
    "              'optimizer': ['Adam'],\n",
    "              'regularization': ['DropOut', 'NodeSampling', 'DropEdge', 'GDC'],\n",
    "              'probability': [0.3, 0.5, 0.7],\n",
    "              'activation': ['relu', 'sigmoid', 'tanh'],\n",
    "              'units': [32, 64],\n",
    "              'convo_type': ['gcn', 'gin']}\n",
    "\n",
    "override_conv_type = 'gin'  # None\n",
    "\n",
    "if override_conv_type is not None:\n",
    "    param_grid['convo_type'] = [override_conv_type]\n",
    "\n",
    "small_sample = list(ParameterGrid(param_grid))[0:1]\n",
    "sample = list(ParameterGrid(param_grid))[0:5]\n",
    "big_sample = list(ParameterGrid(param_grid))[0:20]\n",
    "small_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = GraphPropPredDataset('ogbg-molhiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.3 regularization:DropOut units:32 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.3 regularization:DropOut units:64 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.3 regularization:NodeSampling units:32 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.3 regularization:NodeSampling units:64 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.3 regularization:DropEdge units:32 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.3 regularization:DropEdge units:64 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.3 regularization:GDC units:32 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.3 regularization:GDC units:64 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.5 regularization:DropOut units:32 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.5 regularization:DropOut units:64 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.5 regularization:NodeSampling units:32 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.5 regularization:NodeSampling units:64 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.5 regularization:DropEdge units:32 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.5 regularization:DropEdge units:64 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.5 regularization:GDC units:32 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.5 regularization:GDC units:64 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.7 regularization:DropOut units:32 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.7 regularization:DropOut units:64 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.7 regularization:NodeSampling units:32 \n",
      "Evaluating hpconfig activation:relu convo_type:gin learning_rate:0.0001 num_layers:1 optimizer:Adam probability:0.7 regularization:NodeSampling units:64 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.create_grid_models(big_sample,'ogbg-molhiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating hyperparameter configuration 0 for dataset ogbg-molhiv.\n",
      "138/138 [==============================] - 3s 9ms/step - loss: 2.8602 - auc_2: 0.4701 - binary_accuracy: 0.0557 - precision_2: 0.0317 - recall_2: 0.9769 - val_loss: 0.9333 - val_auc_2: 0.4849 - val_binary_accuracy: 0.6304 - val_precision_2: 0.0246 - val_recall_2: 0.2769\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 0.9359 - auc_2: 0.4794 - binary_accuracy: 0.6302 - precision_2: 0.0240 - recall_2: 0.2692\n",
      "138/138 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "gridS.train_and_evaluate(small_sample, 'ogbg-molhiv')\n",
    "for dataset in datasets: \n",
    "    gridS.train_and_evaluate(sample, 'ogbg-molpcba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the base directory where the experiment results will be saved\n",
    "base_dir = \"experiment_results\"\n",
    "\n",
    "# Define a list of dataset names\n",
    "datasets = [\"dataset_A\", \"dataset_B\", \"dataset_C\"]\n",
    "\n",
    "# Define a dictionary of hyperparameters for each dataset\n",
    "hyperparams_dict = {\n",
    "    \"dataset_A\": {\n",
    "        \"param1\": 0.5,\n",
    "        \"param2\": 10,\n",
    "        \"param3\": \"string\"\n",
    "    },\n",
    "    \"dataset_B\": {\n",
    "        \"param1\": 0.3,\n",
    "        \"param2\": 5,\n",
    "        \"param3\": \"another string\"\n",
    "    },\n",
    "    \"dataset_C\": {\n",
    "        \"param1\": 0.8,\n",
    "        \"param2\": 20,\n",
    "        \"param3\": \"third string\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define a list of hyperparameter configurations to test for each dataset\n",
    "hp_configs = {\n",
    "    \"dataset_A\": [\"hpconfig_0\", \"hpconfig_1\", \"hpconfig_2\"],\n",
    "    \"dataset_B\": [\"hpconfig_0\", \"hpconfig_1\"],\n",
    "    \"dataset_C\": [\"hpconfig_0\"]\n",
    "}\n",
    "\n",
    "# Define the number of times to repeat each hyperparameter configuration\n",
    "num_repeats = 3\n",
    "\n",
    "# Iterate over each dataset\n",
    "for dataset in datasets:\n",
    "    # Define the directory path for the current dataset\n",
    "    dataset_dir = os.path.join(base_dir, dataset)\n",
    "    \n",
    "    # Create the directory for the current dataset\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    \n",
    "    # Define the path for the hyperparameters JSON file\n",
    "    hyperparams_path = os.path.join(dataset_dir, \"hyperparams.json\")\n",
    "    \n",
    "    # Write the hyperparameters dictionary to the JSON file\n",
    "    with open(hyperparams_path, \"w\") as f:\n",
    "        json.dump(hyperparams_dict[dataset], f)\n",
    "    \n",
    "    # Iterate over each hyperparameter configuration for the current dataset\n",
    "    for hp_config in hp_configs[dataset]:\n",
    "        # Define the directory path for the current hyperparameter configuration\n",
    "        hp_config_dir = os.path.join(dataset_dir, hp_config)\n",
    "        \n",
    "        # Create the directory for the current hyperparameter configuration\n",
    "        os.makedirs(hp_config_dir, exist_ok=True)\n",
    "        \n",
    "        # Iterate over the number of repeats for the current hyperparameter configuration\n",
    "        for repeat in range(num_repeats):\n",
    "            # Define the filename for the current repeat\n",
    "            filename = f\"repeat_{repeat}.json\"\n",
    "            \n",
    "            # Define the path for the current JSON file\n",
    "            json_path = os.path.join(hp_config_dir, filename)\n",
    "            \n",
    "            # Define the dictionary to write to the current JSON file\n",
    "            data = {\n",
    "                \"dataset\": dataset,\n",
    "                \"hyperparams\": hyperparams_dict[dataset],\n",
    "                \"hp_config\": hp_config,\n",
    "                \"repeat\": repeat\n",
    "            }\n",
    "            \n",
    "            # Write the dictionary to the current JSON file\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(data, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
